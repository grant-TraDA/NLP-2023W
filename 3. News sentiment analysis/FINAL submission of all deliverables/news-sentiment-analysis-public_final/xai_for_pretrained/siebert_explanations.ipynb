{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo of XAI for sentiment analysis\n",
    "\n",
    "This notebook consists of calculation of attribution using Integrated Gradients method for siebert model. It was inspired by and based on the official tutorial: [https://captum.ai/tutorials/Bert_SQUAD_Interpret](https://captum.ai/tutorials/Bert_SQUAD_Interpret)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from captum.attr import LayerIntegratedGradients, visualization\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:39.100476500Z",
     "start_time": "2023-11-21T21:48:35.693381200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "print(sentiment_analysis(\"I love this!\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:43.913026700Z",
     "start_time": "2023-11-21T21:48:39.100476500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = sentiment_analysis.model\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:43.959929600Z",
     "start_time": "2023-11-21T21:48:43.913026700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = sentiment_analysis.tokenizer\n",
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:43.975526Z",
     "start_time": "2023-11-21T21:48:43.959929600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(inputs, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.053649200Z",
     "start_time": "2023-11-21T21:48:43.975526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.069297300Z",
     "start_time": "2023-11-21T21:48:43.991149200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids  + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.069297300Z",
     "start_time": "2023-11-21T21:48:44.006774900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can input text and attribution label with respect to which the attribution will be calculated.\n",
    "Labels:\n",
    "* 0 - negative\n",
    "* 1 - positive\n",
    "\n",
    "So, for instance, the text below has clearly negative sentiment, but we will calculate the attribution with respect to the positive label. We will obtain info which tokens contribute \"anti\" positive label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"Today is a terrible day and i cant stop crying\"\n",
    "attribution_label = torch.tensor([[1]])\n",
    "\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.069297300Z",
     "start_time": "2023-11-21T21:48:44.038025400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = predict(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "\n",
    "print('Text: ', text)\n",
    "print('Tokens', all_tokens)\n",
    "print('Predicted Sentiment: ', scores)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.346572700Z",
     "start_time": "2023-11-21T21:48:44.053649200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiment_analysis(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.627825Z",
     "start_time": "2023-11-21T21:48:44.346572700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.softmax(scores, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:44.674698900Z",
     "start_time": "2023-11-21T21:48:44.627825Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(predict, model.roberta.embeddings)\n",
    "\n",
    "attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                    target=attribution_label,\n",
    "                                    baselines=ref_input_ids,\n",
    "                                    additional_forward_args=(position_ids,attention_mask),\n",
    "                                    return_convergence_delta=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:51.801861600Z",
     "start_time": "2023-11-21T21:48:44.643452500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "attributions_sum = summarize_attributions(attributions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:51.817483900Z",
     "start_time": "2023-11-21T21:48:51.801861600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vis = visualization.VisualizationDataRecord(\n",
    "    word_attributions=attributions_sum,\n",
    "    pred_prob=torch.max(torch.softmax(scores[0], dim=0)),\n",
    "    pred_class=torch.argmax(scores[0]),\n",
    "    true_class=torch.argmax(scores[0]),\n",
    "    attr_class=str(attribution_label),\n",
    "    attr_score=attributions_sum.sum(),\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=delta)\n",
    "\n",
    "print('\\033[1m', 'Visualizations', '\\033[0m')\n",
    "visualization.visualize_text([vis])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:48:51.864362300Z",
     "start_time": "2023-11-21T21:48:51.817483900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we change the text, the attribution also changes.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"Today is a beautiful day and i cant stop smiling\"\n",
    "attribution_label = torch.tensor([[1]])\n",
    "\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "scores = predict(input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "\n",
    "print('Text: ', text)\n",
    "print('Tokens', all_tokens)\n",
    "print('Predicted Sentiment: ', scores, \" HuggingFace model:\", sentiment_analysis(text))\n",
    "\n",
    "attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                    target=attribution_label,\n",
    "                                    baselines=ref_input_ids,\n",
    "                                    additional_forward_args=(position_ids, attention_mask),\n",
    "                                    return_convergence_delta=True)\n",
    "\n",
    "attributions_sum = summarize_attributions(attributions)\n",
    "vis = visualization.VisualizationDataRecord(\n",
    "    word_attributions=attributions_sum,\n",
    "    pred_prob=torch.max(torch.softmax(scores[0], dim=0)),\n",
    "    pred_class=torch.argmax(scores[0]),\n",
    "    true_class=torch.argmax(scores[0]),\n",
    "    attr_class=str(attribution_label),\n",
    "    attr_score=attributions_sum.sum(),\n",
    "    raw_input_ids=all_tokens,\n",
    "    convergence_score=delta)\n",
    "\n",
    "print('\\033[1m', 'Visualizations', '\\033[0m')\n",
    "visualization.visualize_text([vis])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T21:59:07.675111300Z",
     "start_time": "2023-11-21T21:58:59.594060300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
