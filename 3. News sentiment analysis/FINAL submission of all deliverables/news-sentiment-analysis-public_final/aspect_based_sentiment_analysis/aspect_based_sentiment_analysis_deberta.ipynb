{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../data_preparation/testset_extended.csv'#'../data_preparation/whole_dataset.csv'\n",
    "PATH_TO_SAVE_RESULTS = './test_predicitons_few_models/testset_extended_results_absa_deberta.csv'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you wish to test it yourself, the only thing you need to specify is the location of csv file with already preprocessed data. How to prepare the preprocessed data from raw data will be described in README.md file that can be found in data_preparation directory with all the necessary scripts provided there.\n",
    "\n",
    "It is important to notice, that this data already has NERs extracted and saved to separate column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "from lib.sentiment_analysis_utils import combine_lede_and_text, remove_text_formatting, read_all_news_in_dir\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True, device=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "#print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The computations will be run using GPU in our case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running ABSA (aspect based sentiment analysis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_TO_DATA)#pd.read_csv('../data_preparation/testset.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading the data prepared for absa task."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prepare classifying pipeline\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "#\n",
    "# classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True, device=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using deberta model for ABSA task, which is off the shell provisioned via HuggingFace."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code below runs predictions on the provided dataframe with model specified above. It runs it for each aspect separately (article keywords + extracted NERs)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i, row in df.iterrows():\n",
    "\n",
    "    keywords = ast.literal_eval(row.keywords_lower)\n",
    "    keywords = [keyword.strip() for keyword in keywords]\n",
    "    ners = ast.literal_eval(row.ner_list)\n",
    "    ners = [ner.strip() for ner in ners]\n",
    "\n",
    "\n",
    "    keywords_aspect_sentiment_dict = dict()\n",
    "    for aspect in keywords:\n",
    "        keywords_aspect_sentiment_dict[aspect] = classifier(row.whole_text, text_pair=aspect)\n",
    "\n",
    "\n",
    "    ner_aspect_sentiment_dict = dict()\n",
    "    for aspect in ners:\n",
    "        ner_aspect_sentiment_dict[aspect] = classifier(row.whole_text, text_pair=aspect)\n",
    "\n",
    "    df.loc[i, 'keywords_sentiment'] = [keywords_aspect_sentiment_dict]\n",
    "    df.loc[i, 'ner_sentiment'] = [ner_aspect_sentiment_dict]  #aspect_sentiment_dict\n",
    "\n",
    "    break\n",
    "    # if i % 100 == 0:\n",
    "    #     df.to_csv(f'whole_dataset_results_absa_{i}.csv')#df.to_csv('testset_results_absa.csv')\n",
    "    #     print(i)\n",
    "\n",
    "    # df.to_csv('testset_extended_results_absa.csv')\n",
    "stop = time.time()\n",
    "print('Took', (stop - start)/60, 'minutes')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4.15 minutes deberta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whole of analysis of slightly above 1700 articles took 150 minutes on GPU took. This is caused by the fact, that for many of them lots of NERs has been extracted. This caused a single article to be analyzed multiple times with respect to changing aspect (changing keyword term)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df.loc[[0],['keywords_sentiment']])\n",
    "#pd.display_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(PATH_TO_SAVE_RESULTS)#df.to_csv('testset_results_absa.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
