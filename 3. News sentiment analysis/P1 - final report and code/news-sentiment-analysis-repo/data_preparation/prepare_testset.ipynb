{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook to prepare data for test set. Articles are sampled and later on will be labelled."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline\n",
    "from lib.sentiment_analysis_utils import combine_lede_and_text, remove_text_formatting, read_all_news_in_dir\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = read_all_news_in_dir(os.getcwd() + \"/../data_preparation/raw_data/en/\")\n",
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = combine_lede_and_text(df_en_raw)\n",
    "df_en_raw = remove_text_formatting(df_en_raw)\n",
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform preprocessing using functions from lib. Lede and remaining text of article are firstly combined. Then text formatting is removed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = df_en_raw[df_en_raw['whole_text']!='']\n",
    "df_en_raw = df_en_raw.drop_duplicates(subset=['whole_text'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop duplicates and articles which text is empty."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "df_en_raw = df_en_raw.join(pd.DataFrame(mlb.fit_transform(df_en_raw.categories),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df_en_raw.index))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use MultiLabelBinarizer to extract categories into separate columns which will allow sampling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For English, there are nine categories:\n",
    "  - Advisory - AD\n",
    "  - Arts and Culture - AC\n",
    "  - Around Slovenia - AS\n",
    "  - Business, finance and economy - BE\n",
    "  - Health, environment, science - HE\n",
    "  - Politics - PO\n",
    "  - Roundup - RU\n",
    "  - Schedule of Events - SE\n",
    "  - Sports - ST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "authors = set()\n",
    "for i, row in df_en_raw.iterrows():\n",
    "    tmp = row.byline.split('/')\n",
    "    for author in tmp:\n",
    "        authors.add(author)\n",
    "\n",
    "print(authors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "extracting authors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categories_to_label = ['AC','AS','BE','HE','PO','ST']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From 9 categories, 6 will be labelled. The other are categories which are for example daily digest of the newspaper. Classifying them will not have much sense and just consume resources. This decision can be justified with business expectations which were confirmed by the press agency we were cooperating with."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_ARTICLES_TO_SAMPLE = 9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_testset = pd.DataFrame()\n",
    "for category in categories_to_label:\n",
    "    counter = 0\n",
    "    df_category = pd.DataFrame()\n",
    "    for i, row in df_en_raw.iterrows():\n",
    "        if i in [45, 57, 29]:\n",
    "            continue\n",
    "        if counter == NUM_OF_ARTICLES_TO_SAMPLE:\n",
    "            df_testset = pd.concat([df_testset, df_category])\n",
    "            break\n",
    "\n",
    "        if row[category] == 1 and row[categories_to_label].sum() == 1:\n",
    "            counter+=1\n",
    "            df_category = pd.concat([df_category, df_en_raw.loc[[i]]])\n",
    "\n",
    "\n",
    "duplicate_rows = df_testset.astype(str).duplicated().any()\n",
    "print(duplicate_rows)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Articles from different categories are sampled. From each 9 articles are selected - this can be modified by setting NUM_OF_ARTICLES_TO_SAMPLE to different value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_testset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, grouped_entities=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we have decided to use Babelscape/wikineural-multilingual-ner model as it gave the most promising preliminary results. Mainly as it firstly allowed for grouped entities. (Entities with names composed of multiple words). Secondly, it was multilingual and trained on large corpora. Even though Slovenian was not among the languages it was trained on, having other than English languages helped. Even though articles are in English, naturally there are slovenian entities mentioned quite often. We believe that training on multiple languages still allowed for more accurate assessment when non-English entity was mentioned. (This model was the most sensitive returning on average the most entities, whereas other had tendency to omit some of the entities. + returned names were actually entities.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, row in df_testset.iterrows():\n",
    "    ner_results = ner_pipeline(row.whole_text)\n",
    "    ner_list = set([result['word'] for result in ner_results])\n",
    "\n",
    "    df_testset.at[i, 'ner_list'] = ''\n",
    "    df_testset.at[i, 'ner_list'] = list(ner_list)\n",
    "\n",
    "    keywords_lower = [keyword.lower() for keyword in row.keywords]\n",
    "    df_testset.at[i, 'keywords_lower'] = ''\n",
    "    df_testset.at[i, 'keywords_lower'] = keywords_lower"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Producing the lists of ners and embedding them. keywords are turned to lowercase."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_testset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for category in categories_to_label:\n",
    "    df_category = df_testset.loc[df_testset[category]==1, ['whole_text', 'ner_list', 'keywords_lower']]\n",
    "    df_category.to_excel(f\"./data_sampled_for_testset/test_set_{category}.xlsx\", sheet_name=category)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving articles of each category to separate files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_testset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_testset.to_csv('testset.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving it also to single csv as it will be more convenient to use it in predictions and model evaluation."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
