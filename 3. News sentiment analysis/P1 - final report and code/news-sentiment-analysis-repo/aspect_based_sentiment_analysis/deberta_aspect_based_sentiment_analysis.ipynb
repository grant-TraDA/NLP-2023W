{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# [ATTENTION] PoC notebook. The final one is the aspect_based_sentiment_analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline\n",
    "from lib.sentiment_analysis_utils import combine_lede_and_text, remove_text_formatting, read_all_news_in_dir\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running the computations on GPU for efficiency."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = read_all_news_in_dir(os.getcwd() + \"/../data_preparation/raw_data/en/\")\n",
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above method iterates over files in specified directory, reading and combining the content into single dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = combine_lede_and_text(df_en_raw)\n",
    "df_en_raw = remove_text_formatting(df_en_raw)\n",
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above methods do 2 tasks.\n",
    "\n",
    "First method combines lede with the rest of the article text. Data from STA is organized in such a way that first paragraph of the article is separated from the rest of the text which is usually displayed only under paid subscription (first paragraph for free). We want to analyze both, therefore we combine.\n",
    "\n",
    "Second method is responsible for removing observed by us text formatting attributes (i.e. we want to remove \\n\\n that separates article paragraphs, or html formatting tags like <b> </b> used to display test in bold)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classify polarity (keywords + named entities as aspects)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df_en_raw = df_en_raw.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As this is just PoC of the project, we limit the number of articles to reduce the time for computations. We want to do some testing on smaller batch of data before we test our solution on something computationally expensive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model selected for initial ABSA available at: https://huggingface.co/yangheng/deberta-v3-large-absa-v1.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prepare classifying pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model selected for initial NER: https://huggingface.co/dslim/bert-base-NER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prepare NER pipeline\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The ABSA model requires to specify aspects towards which we want to measure the sentiment. Those needs to be provided as input along with the text that we want to analyze. For this purpose, we are leveraging keywords associated with each article and available in its metadata. We have seen, that those are often reasonable aspects, towards which the sentiment could be investigated and provide interesting insights for end user. However, we also noticed, that they might not cover fully issues mentioned in the article that employees from SLA might be interested in as well. To bring extra value to the project and potentially enrich final results, we decided extended our project by leveraging NER to extract additional aspects from texts towards which the polarity will also be scored."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, row in df_en_raw.iterrows():\n",
    "    keywords = [keyword.lower() for keyword in row.keywords]\n",
    "    keywords_aspect_sentiment_dict = dict()\n",
    "    for aspect in keywords:\n",
    "        keywords_aspect_sentiment_dict[aspect] = classifier(row.whole_text, text_pair=aspect)\n",
    "\n",
    "    df_en_raw.loc[i, 'keywords_sentiment'] = [keywords_aspect_sentiment_dict]\n",
    "\n",
    "    ner_results = ner_pipeline(row.whole_text)\n",
    "    ner_list = [result['word'] for result in ner_results]\n",
    "    ner_aspect_sentiment_dict = dict()\n",
    "    for aspect in ner_list:\n",
    "        ner_aspect_sentiment_dict[aspect] = classifier(row.whole_text, text_pair=aspect)\n",
    "\n",
    "    df_en_raw.loc[i, 'ner_sentiment'] = [ner_aspect_sentiment_dict]  #aspect_sentiment_dict\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw.to_csv('example_results2.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results checked manually on few examples in the context of aspect based sentiment analysis seem reasonable. (as in case of document analysis, we are noc eligible to provide samples of articles, on which we evaluated). However, the preliminary results showed that we have some errors when it comes NER task, which we added. There are some entities extracted, which seem to be erroneous and not even a part of the article's text e.g. ##lo, Go, ##e, V. Which are not valid entities. We will pay attention to that and try to improve. (But still, there are entities recognized correctly, like NATO, Slovenia etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
