{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://huggingface.co/yangheng/deberta-v3-large-absa-v1.1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "from lib.sentiment_analysis_utils import combine_lede_and_text, remove_text_formatting, read_all_news_in_dir\n",
    "\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import torch\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load and preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = read_all_news_in_dir(os.getcwd() + \"/../data_preparation/raw_data/en/\")\n",
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above method iterates over files in specified directory, reading and combining the content into single dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = combine_lede_and_text(df_en_raw)\n",
    "df_en_raw = remove_text_formatting(df_en_raw)\n",
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above methods do 2 tasks.\n",
    "\n",
    "First method combines lede with the rest of the article text. Data from STA is organized in such a way that first paragraph of the article is separated from the rest of the text which is usually displayed only under paid subscription (first paragraph for free). We want to analyze both, therefore we combine.\n",
    "\n",
    "Second method is responsible for removing observed by us text formatting attributes (i.e. we want to remove \\n\\n that separates article paragraphs, or html formatting tags like <b> </b> used to display test in bold)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classify polarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw = df_en_raw.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As this is just PoC of the project, we limit the number of articles to reduce the time for computations. We want to do some testing on smaller batch of data before we test our solution on something computationally expensive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, row in df_en_raw.iterrows():\n",
    "    df_en_raw.loc[i, 'overall_sentiment'] = sentiment_analysis(' '.join(row.whole_text.split(' ')[:350]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_en_raw.to_csv('example_results.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results comment: model seems to provide something reasonable, however, we are still in the process of creating labelled dataset, therefore we will put bigger attention to the evaluation of models in the following weeks. Here is just PoC, we will validate performance in more detail.\n",
    "\n",
    "We have checked manually a few examples (that's why we consider results reasonable), however, we are unable to disclose these sample news articles in public repository."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, we put a hard limit on the number of words in articles (we trim it arbitrary), to fit it to the input of the model that is max 512 token long. (we trim to 350 as result from tokenizer is sometimes longer than number of words). To solve it, we will probably also experiment with 2 approaches: 1. changing the model to one that accepts input of sizes like 4096 tokens 2. summarizing the article with another model before passing for sentiment analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
